# Op

所有操作符的基类，包含以下方法

1. call：将操作符类当做函数调用时执行这个方法
2. compute：计算操作后的结果
3. gradient：计算对输入的导数
4. gradient_as_tuple：将导数以元组的形式返回，这样即便只有一个输入，也可以以a[0]的形式访问，更加统一

# TensorOp

是Op的子类，但也是所有tensor的op的基类

## call函数

它只在Op的基础上实现了call函数，在call函数中调用了`Tensor.make_from_op(self, *args)`

这个函数作用的本质是，假如我们有a和b两个tensor，当我们执行

c=a+b时，本质上是通过重载+号来调用了`AddOp(a,b)`（AddOp是一个Op的一个子类），而这就会调用Op中对`__call__`的重载，即`Tensor.make_from_op(self, *args)`，其中self就是AddOp，args为a和b，这样就通过这两个输入和一种op构建了一个新的tensor，也就是c

# Value

## 属性

value就是tensor的父类，本质上是神经网络的一个节点，而一个节点通常要包含以下信息

1. 操作符`op`
2. 输入`inputs`
3. 数据`cached_data`（输入经过操作符后的输出），目前底层的数据是用numpy的数组实现
4. 是否需要梯度`requires_grad`（是否要更新它）

## 方法

### realize_cached_data

调用这个函数可以得到cached_data，也就是说，构建好了一个节点不代表它的输出被计算好了，一直等待调用了这个函数，才会真正地计算

而这个函数的实现，就是通过调用了op的computer函数，将inputs中的东西全部输入进去，就得到了输出，关键语句如下

```c++
self.cached_data = self.op.compute(
    *[x.realize_cached_data() for x in self.inputs]
)
```

### make_const

这个函数是返回一个tensor，值和当前tensor一样，但是它没有inputs没有op，也就是说是一个完全独立的tensor，在某些情况下可以避免内存的大量浪费

虽然说它是一个独立的tensor，但是它的data是和原来的tensor是共享的，在底层是同一份，因此可以用来执行一些更新值的操作而不会增加额外的没用的计算图

```c++
    def make_const(cls, data, *, requires_grad=False):
        value = cls.__new__(cls)
        value._init(
            None,
            [],
            cached_data=data,
            requires_grad=requires_grad,
            )
        return value
```

### make_fron_op

这就是之前讲的，根据op和输入构建一个tensor，这里会有两个特殊的情况

1. 如果设置了LAZY_MODE，代表不想计算当前节点的输出值，所以不调用`realize_cached_data`，直接返回只包含op和输入的tensor
2. 如果当前节点不需要梯度，那调用detach函数，detach函数其实就是调用了make_const函数

```c++
    def make_from_op(cls, op: Op, inputs: List["Value"]):
        value = cls.__new__(cls)
        value._init(op, inputs)

        if not LAZY_MODE:
            if not value.requires_grad:
                return value.detach()
            value.realize_cached_data()
        return value
```

# Tensor

这是value的子类，是我们真正的神经网络的结点。

## init函数

它在value的基础上又丰富了一些信息，包括device和dtype

首先，它会将输入的数据转为`array_api`对应的数据结构，这是tensor的cached_data底层的数据结构

然后会调用父类的init函数完成基础数据，如op和data的初始化

## backward

每个tensor都有backward函数，通常是会在loss这个tensor调用backward，所以其实loss就是一个tensor罢了，没啥特别的

在调用backward函数时，如果指定了out_grad，那就使用指定的，否则就用1。这个out_grad就是梯度的发起点，意味着loss这里的梯度是1，然后开始反向传播，通过链式求导法则，不断相乘得到前面的tensor的梯度

具体来说，这个操作会通过`compute_gradient_of_variables`实现，在这个函数中，它首先会使用dfs求出拓扑排序，然后以逆拓扑排序的顺序完成反向传播，计算出每个节点的梯度

























